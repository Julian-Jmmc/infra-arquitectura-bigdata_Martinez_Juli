---

# infra-arquitectura-bigdata_Martinez_Juli
 ğŸš€

<div align="center">
  <h1>Infraestructura y Arquitectura para Big Data</h1>
  <h2>
    Proyecto Integrador
    <br><br>
    Parte 1 EA1 - IngestiÃ³n de Datos desde un API ğŸ“Š  
    ã…¤ <br><br>
    Parte 2 EA2 - Preprocesamiento y Limpieza de Datos simulando una Plataforma de Big Data en la Nube ğŸ”ğŸš€
    ã…¤ <br><br>
    Parte 3 EA3 - Enriquecimiento de Datos simulando una Plataforma de Big Data en la Nube ğŸŒğŸ“ˆ
  </h2>
</div>

---

Este proyecto es un flujo completo de datos de **COVID-19** que abarca desde la ingesta hasta el preprocesamiento, limpieza y enriquecimiento. En la fase **EA1**, se extraen datos histÃ³ricos desde la API pÃºblica del **COVID Tracking Project** ğŸ˜·, se almacenan en **SQLite** ğŸ’¾, se genera un archivo **Excel** de muestra ğŸ“ˆ y se crea un reporte de auditorÃ­a ğŸ“‹. AdemÃ¡s, se implementa un flujo de integraciÃ³n continua en **GitHub Actions** ğŸ¤– que automatiza la ejecuciÃ³n del script y la construcciÃ³n de la imagen Docker. ğŸ³

---

> **Importante:**  
> El COVID Tracking Project finalizÃ³ la recolecciÃ³n de datos el **7 de marzo de 2021** ğŸ“†, por lo que la informaciÃ³n disponible es histÃ³rica y no se actualiza mÃ¡s allÃ¡ de esa fecha.

---

En la **fase 2** de este proyecto, ğŸ” a partir de los datos obtenidos se simulan condiciones de "datos sucios" mediante la duplicaciÃ³n de registros ğŸ“‘, la inserciÃ³n de valores nulos âš ï¸, la forzada conversiÃ³n de tipos ğŸ”„ y la creaciÃ³n de columnas adicionales â•. Posteriormente, se aplican operaciones de limpieza y transformaciÃ³n: se eliminan duplicados ğŸ—‘ï¸, se convierten columnas a formatos numÃ©ricos (imputando valores nulos con la mediana) ğŸ”¢, se eliminan columnas redundantes âœ‚ï¸, se aÃ±aden columnas de auditorÃ­a (fecha y hora) â°, se renombran las columnas a nombres en espaÃ±ol ğŸ“ y se calculan indicadores clave como la tasa de positividad ğŸ“Š y la tasa de mortalidad âš°ï¸. Como resultado, se genera un archivo CSV con los datos limpios y otro con los datos sucios para muestra ğŸ“‚, se crea un informe de auditorÃ­a detallado ğŸ“‹ y se actualiza la base de datos con 2 tablas nuevas la tabla limpia y la tabla sucia ğŸ—„ï¸.

---

En la **fase 3 (EA3)**, ğŸŒ se enriquece el dataset base integrando datos externos de delitos informÃ¡ticos y un inventario anual de bovinos en Antioquia, procesando y unificando esta informaciÃ³n con el dataset limpio de COVID-19 para generar un conjunto de datos mÃ¡s completo y Ãºtil para anÃ¡lisis.

---

## 1. DescripciÃ³n General ğŸŒŸ

### EA1 â€“ Ingesta de Datos
Este proyecto realiza las siguientes tareas:

* ğŸ“¥ **Descarga de datos:**  
  Descarga datos histÃ³ricos de COVID-19 de los EE.UU. desde la API del COVID Tracking Project.
  - **PÃ¡gina web del API:**  
    [https://covidtracking.com/data/api](https://covidtracking.com/data/api)
  - **Formato JSON:**  
    [https://api.covidtracking.com/v1/us/daily.json](https://api.covidtracking.com/v1/us/daily.json)
  - **Formato CSV (opcional):**  
    [https://api.covidtracking.com/v1/us/daily.csv](https://api.covidtracking.com/v1/us/daily.csv)
  - **Recurso adicional:**  
    [Las mejores APIs pÃºblicas para practicar](https://ed.team/blog/las-mejores-apis-publicas-para-practicar)

* ğŸ—„ï¸ **Base de datos SQLite:**  
  Crea (o actualiza) una base de datos para almacenar la informaciÃ³n.

* ğŸ“Š **GeneraciÃ³n de archivo Excel:**  
  Genera un archivo Excel con los Ãºltimos 50 registros a modo de muestra.

* ğŸ“ **Informe de auditorÃ­a:**  
  Crea un informe de auditorÃ­a (archivo de texto) comparando la cantidad de registros en la API y en la base de datos.

* ğŸš€ **AutomatizaciÃ³n:**  
  Automatiza la ejecuciÃ³n de la ingesta y la construcciÃ³n de la imagen Docker a travÃ©s de GitHub Actions.

---

Al finalizar, tendrÃ¡s:

* ğŸ—ƒï¸ Una base de datos ingestion.db.
* ğŸ“‘ Un archivo Excel ingestion.xlsx con 50 registros en la carpeta xlsx.
* ğŸ“œ Un archivo de auditorÃ­a ingestion.txt con un resumen de los datos procesados.
* ğŸ¤– Un pipeline en GitHub Actions que ejecuta estos pasos automÃ¡ticamente.

---

## 2. Acerca de la API (COVID Tracking Project) ğŸ“¡

El COVID Tracking Project fue una iniciativa que recopilÃ³ datos de COVID-19 en EE.UU. hasta el **7 de marzo de 2021**. A partir de esa fecha, la recolecciÃ³n de datos se detuvo y la informaciÃ³n quedÃ³ congelada como un registro histÃ³rico. Algunas consideraciones importantes:

* **URL Base:**  
  [https://api.covidtracking.com](https://api.covidtracking.com)
* **Endpoint de Datos HistÃ³ricos (JSON):**  
  [https://api.covidtracking.com/v1/us/daily.json](https://api.covidtracking.com/v1/us/daily.json)
  - *TambiÃ©n disponible en CSV:*  
    [https://api.covidtracking.com/v1/us/daily.csv](https://api.covidtracking.com/v1/us/daily.csv)
* **Formato de Respuesta:**  
  JSON
* **Licencia:**  
  CC BY 4.0 https://covidtracking.com/about-data/license

**Campos Principales:**
* date: Fecha del reporte
* positive: Casos positivos acumulados
* death: Fallecimientos confirmados o probables
* hospitalizedCurrently: Hospitalizados
* totalTestResults: NÃºmero total de tests reportados
* positiveIncrease: Nuevos casos
* deathIncrease: Nuevas muertes
* lastModified: Ãšltima fecha/hora de actualizaciÃ³n

---

## 3. EA2 â€“ Limpieza y Preprocesamiento de Datos  
A partir de la base de datos generada en EA1 se simulan "datos sucios" y se aplican las siguientes operaciones:

* **SimulaciÃ³n de datos sucios:**  
  - ğŸ” **DuplicaciÃ³n de un 20% de registros.**
  - âš ï¸ **InserciÃ³n de valores nulos en columnas numÃ©ricas.**
  - â• **CreaciÃ³n de columnas adicionales** (por ejemplo, 'registro_nulo', columnas duplicadas como 'fecha_duplicada', 'positive_duplicada', 'death_duplicada', 'columna_inutil_1', 'columna_inutil_2' y 'fecha_sucia').
  - ğŸ”„ **Forzado error de tipo** en las columnas 'positive' y 'death' (convirtiendo un 7% de los valores a cadena).

* **Limpieza y transformaciÃ³n:**  
  - ğŸ—‘ï¸ **EliminaciÃ³n de duplicados.**
  - ğŸ”¢ **ConversiÃ³n de columnas a tipos numÃ©ricos** e imputaciÃ³n de valores nulos con la mediana.
  - ğŸ§¹ **Relleno de valores faltantes** en 'registro_nulo' con "Sin Dato".
  - âœ‚ï¸ **EliminaciÃ³n de columnas inservibles y duplicadas** (como 'fecha_duplicada', 'positive_duplicada', 'death_duplicada', 'columna_inutil_1', 'columna_inutil_2' y 'fecha_sucia').
  - â° **GeneraciÃ³n de nuevas columnas de auditorÃ­a:**  
    Se aÃ±aden columnas 'anio_auditoria', 'mes_auditoria', 'dia_auditoria' y 'fecha_completa_auditoria' con la fecha y hora actuales (por ejemplo, 2025-03-10 y 2025-03-10 13:48:16).
  - ğŸ“ **Renombrado de columnas a nombres en espaÃ±ol.**
  - ğŸ“Š **CÃ¡lculo de indicadores clave:**  
    - **Tasa de positividad:** Calculada como (casos_positivos / total_resultados) * 100.  
    - **Tasa de mortalidad:** Calculada como (fallecidos / casos_positivos) * 100.

* **AuditorÃ­a del Proceso de Limpieza:**  
  Se genera un informe de auditorÃ­a que detalla el estado de los datos antes y despuÃ©s de la limpieza

* **Salida:**  
  - ğŸ“ **Archivos CSV:** Se generan 2 archivos uno con los datos limpios y otro con los datos sucios (Tabla_Datos_Limpios.csv, Tabla_Datos_Sucios.csv).  
  - ğŸ“‹ **Informe de auditorÃ­a detallado:** Se crea el archivo Informe_Limpieza.txt.  
  - ğŸ—„ï¸ **ActualizaciÃ³n de la base de datos:** Se agrega la nueva tabla covid_data_cleaned y tambiÃ©n se agrega covid_data_dirty para muestra de los datos sucios.

---

## 4. EA3 â€“ Enriquecimiento de Datos ğŸ”ğŸ“Š  

En esta fase se integra informaciÃ³n adicional proveniente de fuentes externas para complementar los datos de COVID-19. En particular, se realiza lo siguiente:  

### ğŸŒ Fuentes Externas:  
- **ğŸ–¥ï¸ Delitos InformÃ¡ticos (CSV):** https://www.datos.gov.co/Justicia-y-Derecho/Delitos-Informaticos-V1/wxd8-ucns/about_data  
  Se extrae informaciÃ³n de un archivo CSV, renombrando y normalizando sus columnas (por ejemplo, se renombra `MUNICIPIO_HECHO` a `municipio`).  

- **ğŸ„ğŸ“¡ Inventario Anual de Bovinos (API):** https://www.datos.gov.co/resource/fy9z-8zxt.json
- https://www.datos.gov.co/Agricultura-y-Desarrollo-Rural/Inventario-anual-de-Bovinos-en-Antioquia-desde-200/fy9z-8zxt/about_data
  Se consulta una API que provee informaciÃ³n sobre bovinos en Antioquia.  
  - ğŸ”„ Se renombra la columna `MUNICIPIO` a `municipio` y se normalizan espacios y formatos.  
  - ğŸŒ¿ Se procesan las columnas relacionadas con pastos: `pasto_mejorado`, `pasto_natural`, `pasto_corte` y `Total Pastos (ha)` (renombrada a `total_pastos`).  
  - ğŸ› ï¸ Los valores nulos en estas columnas se imputan con la mediana de cada una, garantizando asÃ­ que la informaciÃ³n no presente huecos que afecten el anÃ¡lisis.  

### ğŸ”— IntegraciÃ³n (Merge):  
- Se realiza un **`merge` (inner join)** entre las dos fuentes externas utilizando la columna comÃºn `municipio` ğŸ™ï¸, lo que permite conservar Ãºnicamente los registros que coinciden en ambas fuentes.  

### ğŸ“Œ IntegraciÃ³n con el Dataset Base:  
- ğŸ“Š Dado que el dataset base de COVID-19 (EA2) no posee una clave geogrÃ¡fica en comÃºn, se toma una muestra representativa de **444 filas** tanto del dataset base como del resultado de la integraciÃ³n de las fuentes externas.  
- ğŸ—ï¸ Finalmente, se concatena horizontalmente la muestra de COVID-19 con la muestra integrada, generando el dataset final enriquecido.  

### ğŸ“ Evidencias Generadas:  
- **ğŸ“œ Archivo CSV Enriquecido:**  
  Se genera un archivo con el dataset final que contiene tanto la informaciÃ³n original de COVID-19 como los datos enriquecidos de las fuentes externas.  
- **ğŸ“„ Reporte de AuditorÃ­a:**  
  Se crea un informe (archivo TXT) que detalla la cantidad de registros y columnas en cada fuente, las operaciones de merge realizadas, y se especifica que las columnas relacionadas con pastos, incluida `total_pastos`, fueron procesadas para imputar valores nulos con la mediana.  

---

## 5. Requerimientos Previos âœ…

Antes de ejecutar el proyecto, asegÃºrate de contar con:

* ğŸ **Python 3.9 o superior** (se recomienda 3.13.2).
* ğŸŒ **Git** (para clonar el repositorio).
* ğŸ³ **(Opcional) Docker**, si deseas ejecutar el proyecto en contenedor.
* ğŸ”— **ConexiÃ³n a Internet** para descargar los datos.
* ğŸ¤– **Cuenta en GitHub** (para la automatizaciÃ³n con GitHub Actions).

---

## 6. Clonar o Descargar el Proyecto ğŸ“‚

### ğŸ”¹ Clonar con Git  
Ejecuta el siguiente comando en tu terminal:  

```bash
git clone https://github.com/Alexis-Machado/infra-arquitectura-bigdata_Alexis_Machado.git
```

---

> **Nota:**  
> Las carpetas auditoria/, db/ y xlsx/ se crean vacÃ­as en el repositorio, pero el cÃ³digo las poblarÃ¡ en tiempo de ejecuciÃ³n.  
> No te preocupes si las ves vacÃ­as inicialmente. ğŸ—‚ï¸

---

## 7. CreaciÃ³n y ActivaciÃ³n de un Entorno Virtual (Recomendado) ğŸ› ï¸

Para mantener organizadas las dependencias del proyecto y evitar conflictos con otras instalaciones de Python, se recomienda usar un entorno virtual.

### ğŸ”¹ Crear el entorno virtual  
Ejecuta el siguiente comando dentro de la carpeta del proyecto:  

```bash
python -m venv venv
```

ğŸ”¹ Activar el entorno virtual

ğŸ”¹ En Windows ğŸ–¥ï¸

```bash
venv\Scripts\activate
```

ğŸ”¹ En Linux/Mac ğŸ’»

```bash
source venv/bin/activate
```

ğŸ”¹ Verificar la activaciÃ³n âœ…  
Si el entorno se activÃ³ correctamente, deberÃ­as ver **(venv)** al inicio de tu lÃ­nea de comandos.

---

## 8. InstalaciÃ³n de Dependencias y Uso de setup.py ğŸ“¦

Para ejecutar el proyecto correctamente y facilitar su instalaciÃ³n, es necesario instalar las bibliotecas requeridas o utilizar setup.py.

### ğŸ”¹ InstalaciÃ³n de dependencias  

Ejecuta los siguientes comandos en la raÃ­z del proyecto (asegÃºrate de tener el entorno virtual activo si lo estÃ¡s usando):  

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

Esto instalarÃ¡ todas las bibliotecas necesarias para el proyecto (por ejemplo: requests, pandas, openpyxl, etc.). âœ…

## ğŸ”¹ Recomendado Uso de setup.py (Empaquetado e InstalaciÃ³n)

El proyecto incluye un setup.py que permite instalarlo como un paquete Python local.

El archivo setup.py ahora estÃ¡ actualizado para reflejar la versiÃ³n EA2

```python
from setuptools import setup, find_packages

setup(
    name="infra-arquitectura-bigdata_Alexis_Machado",
    version="3.0.0",
    author="Jhon Alexis Machado Rodriguez",
    author_email="jmachadoa12@gmail.com",
    description="EA3 Proyecto Integrador: Enriquecimiento de Datos simulando una Plataforma de Big Data en la Nube. ğŸ”ğŸš€",
    py_modules=["EA3_Enriquecimiento_de_Datos_simulando_una_Plataforma_de_Big_Data_en_la_Nube"],
    install_requires=[
        'requests',
        "pandas",
        "openpyxl"
    ]
)

```

AsegÃºrate de estar en el directorio raÃ­z del proyecto (donde estÃ¡ setup.py).  
Para instalarlo, ejecuta:

```bash
pip install -e .
```

Esto instalarÃ¡ el paquete en tu entorno Python y permitirÃ¡ modificar el cÃ³digo sin necesidad de reinstalarlo.

---

## 9. EjecuciÃ³n del Proyecto (Local) ğŸš€

### Ejecutar Ingesta (EA1)

Para ejecutar el primer paso del proyecto, asegÃºrate de tener el entorno virtual activo (si lo estÃ¡s utilizando) o haber instalado las dependencias globalmente. Luego, ejecuta el script principal: 

```bash
python src/bigdata/ingestion.py
```

*El script descargarÃ¡ los datos, los almacenarÃ¡ en SQLite, generarÃ¡ el archivo Excel y el informe de auditorÃ­a correspondiente.*<br><br>

### ğŸ”¹ Ejemplo de salida en consola
Al ejecutar el script, verÃ¡s un proceso similar a este:

```bash
Comenzando la Ingesta de Datos de COVID-19...
Â¡Datos obtenidos! Se han obtenido 420 Registros HistÃ³ricos del COVID-19 en EE.UU
Los Datos del COVID-19 se han Almacenado en la Base de Datos SQLite
Archivo de Excel Generado en: ...\src\bigdata\static\xlsx\ingestion.xlsx
AuditorÃ­a Generada en: ...\src\bigdata\static\auditoria\ingestion.txt
Â¡Proceso Finalizado con Ã‰xito!
```

#### ğŸ”¹ Archivos generados al finalizar

âœ” Base de datos SQLite:  
ğŸ“‚ Se crearÃ¡ (o actualizarÃ¡) el archivo ingestion.db en src/bigdata/static/db/.

âœ” Archivo Excel de muestra:  
ğŸ“Š Se generarÃ¡ un archivo ingestion.xlsx con 50 registros en src/bigdata/static/xlsx/.

âœ” Informe de auditorÃ­a:  
ğŸ“ Un archivo ingestion.txt estarÃ¡ disponible en src/bigdata/static/auditoria/.

Â¡Listo! El proceso de ingesta y almacenamiento de datos ha sido completado con Ã©xito. ğŸ‰

---

### Ejecutar Limpieza y Preprocesamiento (EA2)

Para ejecutar la fase de limpieza y preprocesamiento, asegÃºrate de haber completado la ingesta (EA1) y de tener el entorno virtual activo (o las dependencias instaladas globalmente). Luego, ejecuta el siguiente comando:

```bash
python src/bigdata/cleaning.py
```

*El script realizarÃ¡ lo siguiente:*

* ğŸ“¥ **Carga de datos:** Extrae los datos desde ingestion.db.
* ğŸ” **SimulaciÃ³n de datos "sucios":**  
  - DuplicaciÃ³n de un **20%** de registros.  
  - InserciÃ³n de valores nulos en columnas numÃ©ricas.  
  - CreaciÃ³n de columnas adicionales (por ejemplo, 'registro_nulo', columnas duplicadas y 'fecha_sucia').  
  - Forzado error de tipo en 'positive' y 'death' (7% de los valores convertidos a cadena).
* ğŸ§¹ **Limpieza y transformaciÃ³n:**  
  - EliminaciÃ³n de duplicados.  
  - ConversiÃ³n de columnas a tipos numÃ©ricos, imputando valores nulos con la mediana.  
  - Relleno de valores faltantes en 'registro_nulo' con "Sin Dato".  
  - EliminaciÃ³n de columnas inservibles (duplicadas y de error).  
  - GeneraciÃ³n de nuevas columnas de auditorÃ­a con la fecha y hora actuales.  
  - Renombrado de columnas a nombres en espaÃ±ol.  
  - CÃ¡lculo de indicadores: tasa de positividad y tasa de mortalidad.
* ğŸ“‹ **AuditorÃ­a del proceso:**  
  Se genera un informe de auditorÃ­a que detalla el estado inicial y final del proceso de limpieza

### ğŸ”¹ Ejemplo de salida en consola

Al ejecutar el script, verÃ¡s un proceso similar a este:

```bash
Iniciando Proceso de Limpieza y Preprocesamiento de Datos...
Datos ExtraÃ­dos de la Base de Datos: 420 Registros
Datos Sucios Simulados: 504 Registros (Incluye Duplicados y Valores Nulos)
Archivo CSV de Datos Sucios Generado en: .../src/bigdata/static/xlsx/Tabla_Datos_Sucios.csv
Tabla 'covid_data_dirty' creada en la Base de Datos con los Datos Sucios.
Operaciones de Limpieza y TransformaciÃ³n Aplicadas
Archivo CSV de datos limpios generado en: .../src/bigdata/static/xlsx/Tabla_Datos_Limpios.csv
Archivo de auditorÃ­a generado en: .../src/bigdata/static/auditoria/Informe_Limpieza.txt
Tabla 'covid_data_cleaned' Creada/Actualizada en la Base de Datos con los Datos Limpios.
Â¡Preprocesamiento y Limpieza de Datos Completado con Ã‰xito!
```

#### ğŸ”¹ Archivos generados al finalizar

âœ” **CSV de datos limpios:**  
ğŸ“‚ Se generarÃ¡ el archivo Tabla_Datos_Limpios.csv en src/bigdata/static/xlsx/.

âœ” **CSV de datos sucios:**  
ğŸ“‚ Se generarÃ¡ el archivo Tabla_Datos_Sucios.csv en src/bigdata/static/xlsx/.

âœ” **Informe de auditorÃ­a de limpieza:**  
ğŸ“ Se crearÃ¡ el archivo Informe_Limpieza.txt en src/bigdata/static/auditoria/.

âœ” **ActualizaciÃ³n de la base de datos:**  
ğŸ—ƒï¸ Se agregarÃ¡n las nuevas tablas covid_data_cleaned y covid_data_dirty  en src/bigdata/static/db/ingestion.db.

Â¡Listo! El proceso de limpieza y preprocesamiento se ha completado con Ã©xito. ğŸ‰

---

### Ejecutar Enriquecimiento (EA3)

Para ejecutar la fase de enriquecimiento, asegÃºrate de haber completado las fases EA1 y EA2, y de tener el entorno virtual activo (o las dependencias instaladas globalmente). Luego, ejecuta el siguiente comando:

```bash
python src/bigdata/enrichment.py
```

*El script realizarÃ¡ lo siguiente:*

- ğŸ“¥ **Carga del dataset base:** Extrae los datos limpios desde `ingestion.db` (tabla `covid_data_cleaned`).
- ğŸ“‚ **Lectura de fuentes externas:**  
  - Carga el archivo CSV de Delitos InformÃ¡ticos (`Delitos_Informaticos.csv`) y renombra columnas (ejemplo: `MUNICIPIO_HECHO` a `municipio`).  
  - Consulta la API de Inventario Anual de Bovinos en Antioquia, renombra `MUNICIPIO` a `municipio`, procesa columnas de pastos (`pasto_mejorado`, `pasto_natural`, `pasto_corte`, `total_pastos`) e imputa valores nulos con la mediana.
- ğŸ”— **IntegraciÃ³n de fuentes externas:** Realiza un `merge` (inner join) entre las fuentes externas usando la columna `municipio`.
- â• **IntegraciÃ³n con el dataset base:** Toma una muestra de 444 filas del dataset base y del resultado del merge, y los concatena horizontalmente.
- ğŸ“¤ **ExportaciÃ³n:** Genera un archivo CSV enriquecido (`datos_enriquecidos.csv`) y un reporte de auditorÃ­a (`reporte_enriquecimiento.txt`).

### ğŸ”¹ Ejemplo de salida en consola

Al ejecutar el script, verÃ¡s un proceso similar a este:

```bash
=== Iniciando Proceso de Enriquecimiento (EA3) ===
Cargando dataset base limpio desde la base de datos (covid_data_cleaned)...
Dataset limpio cargado: 444 registros.
Leyendo archivo CSV de Delitos InformÃ¡ticos...
Delitos InformÃ¡ticos leÃ­dos: 56502 registros.
Consultando API de Inventario Anual de Bovinos en Antioquia...
Inventario Bovinos desde API: 1000 registros.

Dataset enriquecido exportado en: ..\src\bigdata\static\xlsx\datos_enriquecidos.csv

Reporte de enriquecimiento generado en: ..\src\bigdata\static\auditoria\reporte_enriquecimiento.txt

=== Proceso de Enriquecimiento Finalizado ===
```

#### ğŸ”¹ Archivos generados al finalizar

âœ” **CSV enriquecido:**  
ğŸ“‚ Se generarÃ¡ el archivo `datos_enriquecidos.csv` en `src/bigdata/static/xlsx/`.

âœ” **Informe de auditorÃ­a de enriquecimiento:**  
ğŸ“ Se crearÃ¡ el archivo `reporte_enriquecimiento.txt` en `src/bigdata/static/auditoria/`.

Â¡Listo! El proceso de enriquecimiento se ha completado con Ã©xito. ğŸ‰

---

## 10. EjecuciÃ³n del Proyecto con Docker ğŸ³ (Opcional)

El **Dockerfile** ha sido actualizado para ejecutar los tres scripts (ingesta, limpieza y enriquecimiento):

```dockerfile
# Usamos la imagen oficial de Python 3.9 como base
FROM python:3.9

# Definimos el directorio de trabajo dentro del contenedor
WORKDIR /app

# Copiamos el archivo de dependencias (requirements.txt) al contenedor
COPY requirements.txt .

# Instalamos las dependencias
RUN pip install --no-cache-dir -r requirements.txt

# Copiamos el cÃ³digo fuente al contenedor dentro del directorio /app/src
COPY src/ ./src

# Ejecutamos en secuencia el script de ingesta de EA1, luego el de preprocesamiento y limpieza de EA2 y por Ãºltimo el de enriquecimiento de EA3
CMD ["sh", "-c", "python src/bigdata/ingestion.py && python src/bigdata/cleaning.py && python src/bigdata/enrichment.py"]

```

### Construir la imagen Docker

Si prefieres ejecutar todo dentro de un contenedor Docker, sigue estos pasos:

Crear la imagen Docker (en la raÃ­z del proyecto, donde estÃ¡ el Dockerfile):

```bash
docker build -t bigdata-ingestion .
```

### Ejecutar el contenedor

```bash
docker run --name bigdata_container bigdata-ingestion
```

Este comando crearÃ¡ un contenedor llamado `bigdata_container` que ejecutarÃ¡ los scripts `ingestion.py`, `cleaning.py` y `enrichment.py` en secuencia.

<br>

Para ver los archivos generados (base de datos, Excel, CSV, auditorÃ­as) dentro del contenedor, puedes montar un volumen o copiar los archivos al host. Por ejemplo, para montar el directorio actual al contenedor:

```bash
docker run -v ${PWD}/src/bigdata/static:/app/src/bigdata/static bigdata-ingestion
```

Si el contenedor ya se ejecutÃ³ y deseas copiar los archivos generados manualmente a tu sistema local, usa:

```bash
docker cp bigdata_container:/app/src/bigdata/static ./src/bigdata/static
```

DespuÃ©s de la ejecuciÃ³n, deberÃ­as ver los siguientes archivos en `src/bigdata/static/`:

* **DB:** `db/ingestion.db` (incluye las tablas `covid_data_cleaned` y `covid_data_dirty`).
* **Excel:** `xlsx/ingestion.xlsx`
* **CSV de datos limpios y sucios:** `xlsx/Tabla_Datos_Limpios.csv` | `xlsx/Tabla_Datos_Sucios.csv`
* **CSV enriquecido:** `xlsx/datos_enriquecidos.csv`
* **AuditorÃ­as:**  
  - `auditoria/ingestion.txt` (de ingesta)  
  - `auditoria/Informe_Limpieza.txt` (de limpieza)  
  - `auditoria/reporte_enriquecimiento.txt` (de enriquecimiento)

Â¡Listo! Ahora tienes tu proyecto ejecutÃ¡ndose en Docker con todos los archivos generados correctamente. ğŸš€

---

## 11. AutomatizaciÃ³n con GitHub Actions ğŸ¤–

Este proyecto incluye un flujo de trabajo en `.github/workflows/main.yml` que automatiza:

* ğŸ“¥ **IngestiÃ³n de Datos desde un API**: Ejecuta el script `ingestion.py` (EA1).
* ğŸ”„ **Preprocesamiento y Limpieza de Datos**: Ejecuta el script `cleaning.py` (EA2).
* ğŸŒ **Enriquecimiento de Datos**: Ejecuta el script `enrichment.py` (EA3).
* ğŸ“¤ **Commit automÃ¡tico**: Guarda los cambios en la base de datos, Excel, CSV y auditorÃ­as si hay modificaciones.
* ğŸ³ **ConstrucciÃ³n y ejecuciÃ³n de Docker**: Crea y ejecuta la imagen del contenedor.<br><br>

### ğŸ”¹ 11.1 Estructura del Flujo de Trabajo

El archivo de configuraciÃ³n `.github/workflows/main.yml` tiene el siguiente contenido actualizado:

```yaml
name: ETL Pipeline Automation

on:
  push:
    branches:
      - main

jobs:
  ingestion:
    runs-on: ubuntu-latest

    steps:
      - name: ğŸ›ï¸ Checkout del repositorio
        uses: actions/checkout@v3

      - name: ğŸ”„ Rebase antes de ingestar
        run: |
          git pull --rebase origin main

      - name: ğŸ Configurar Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: ğŸ“¦ Instalar dependencias
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: ğŸš€ Ejecutar script de ingesta (EA1)
        run: python src/bigdata/ingestion.py

      - name: ğŸš€ Ejecutar script de limpieza (EA2)
        run: python src/bigdata/cleaning.py

      - name: ğŸš€ Ejecutar script de enriquecimiento (EA3)
        run: python src/bigdata/enrichment.py

      - name: ğŸ“‚ Configurar Git
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"

      - name: ğŸ“¤ Hacer commit de los cambios
        run: |
          git add .
          git commit -m "ActualizaciÃ³n AutomÃ¡tica de Datos (EA1, EA2 y EA3) âœ…ğŸ‰" || echo "No hay cambios para commitear"
          git push https://${{ secrets.GITHUB_TOKEN }}@github.com/Alexis-Machado/infra-arquitectura-bigdata_Alexis_Machado.git
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  docker-build:
    needs: ingestion
    runs-on: ubuntu-latest

    steps:
      - name: ğŸ›ï¸ Checkout del repositorio
        uses: actions/checkout@v3

      - name: ğŸ³ Construir y ejecutar contenedor Docker
        run: |
          docker build -t bigdata-ingestion .
          docker run --rm bigdata-ingestion
```

Esta configuraciÃ³n automatiza la ingesta, limpieza y enriquecimiento de datos, realizando un commit automÃ¡tico si hay cambios y, finalmente, construyendo y ejecutando un contenedor Docker.

Cada vez que se realice un push a la rama `main`, se ejecutarÃ¡n de forma secuencial los scripts de ingesta (EA1), limpieza (EA2) y enriquecimiento (EA3), seguido de la construcciÃ³n y ejecuciÃ³n del contenedor Docker.

---

### ğŸ”¹ 11.2 ExplicaciÃ³n del Flujo

ğŸ¯ **Evento de disparo:**

Cada vez que se hace un push a la rama `main`, se ejecutan los siguientes jobs:

* **ğŸ“¥ Checkout del repositorio:** Se descarga el repositorio.  
* **ğŸ ConfiguraciÃ³n de Python:** Se configura Python 3.9.  
* **ğŸ“¦ InstalaciÃ³n de dependencias:** Se instalan las dependencias indicadas en `requirements.txt`.  
* **ğŸš€ EjecuciÃ³n del script de ingesta (EA1):**  
  Se ejecuta `ingestion.py`, el cual descarga los datos de COVID-19, los almacena en SQLite, genera un archivo Excel y un informe de auditorÃ­a.  
* **ğŸ”„ EjecuciÃ³n del script de limpieza (EA2):**  
  A continuaciÃ³n, se ejecuta `cleaning.py`, que carga los datos, simula datos "sucios", aplica las operaciones de limpieza y transformaciÃ³n, exporta los CSV con los datos limpios y sucios, genera un informe de auditorÃ­a de limpieza y actualiza la base de datos con las nuevas tablas `covid_data_cleaned` y `covid_data_dirty`.  
* **ğŸŒ EjecuciÃ³n del script de enriquecimiento (EA3):**  
  Luego, se ejecuta `enrichment.py`, que integra datos externos (Delitos InformÃ¡ticos e Inventario Bovinos), los combina con una muestra del dataset limpio de COVID-19, y genera un CSV enriquecido y un reporte de auditorÃ­a.  
* **ğŸ“ ConfiguraciÃ³n de Git:** Se configuran los datos de Git para realizar commits automÃ¡ticos desde GitHub Actions.  
* **ğŸ“¤ Commit automÃ¡tico:** Se realiza un commit de los cambios generados (nuevos datos, Excel, auditorÃ­as, CSV, etc.) si existen, y se hace push al repositorio.

ğŸ³ **docker-build** (depende de ingestion)  
* **ğŸ“¥ Checkout del repositorio:** Se descarga el repositorio.  
* **ğŸ› ï¸ ConstrucciÃ³n de la imagen Docker:** Se ejecuta `docker build` para construir la imagen.  
* **ğŸš¢ EjecuciÃ³n del contenedor Docker:** Se ejecuta `docker run`, lo que lanza un contenedor que ejecuta secuencialmente los scripts de ingesta (EA1), limpieza (EA2) y enriquecimiento (EA3).

<br><br>

ğŸš€ Â¡Listo! Con este flujo de trabajo, la ingesta de datos, el preprocesamiento, la limpieza, el enriquecimiento y la construcciÃ³n del contenedor Docker se ejecutarÃ¡n automÃ¡ticamente cada vez que subas cambios al repositorio. ğŸ‰

---

### ğŸ”¹ 11.3 CÃ³mo Personalizarlo ğŸ”§

Cambiar la rama en la que se dispara el flujo:

Si deseas que la automatizaciÃ³n se ejecute en otra rama, edita:

```yaml
on:
  push:
    branches:
      - main
```

ğŸ“Œ Ajustar la versiÃ³n de Python:

Si necesitas otra versiÃ³n de Python, cambia:

```yaml
with:
  python-version: '3.9'
```

ğŸ”— Configurar la URL del repositorio:  
AsegÃºrate de que el paso `git push` apunte a tu repositorio:

```yaml
git push https://${{ secrets.GITHUB_TOKEN }}@github.com/TU_USUARIO/TU_REPO.git main
```

ğŸ“Š Ver logs y resultados:

Para revisar la ejecuciÃ³n, dirÃ­gete a la pestaÃ±a **Actions** en tu repositorio de GitHub.
<br><br>

ğŸš€ Â¡Listo! Con este flujo de trabajo automatizado, la ingesta de datos, el preprocesamiento, la limpieza, el enriquecimiento y la construcciÃ³n del contenedor Docker se ejecutarÃ¡n automÃ¡ticamente cada vez que realices un push al repositorio. ğŸ‰

---

## 12. ConclusiÃ³n ğŸ¯

Con este proyecto se integra un pipeline completo de ETL para datos de COVID-19:

* **ExtracciÃ³n (EA1):**  
  Descarga de datos desde la API del COVID Tracking Project, almacenamiento en SQLite y generaciÃ³n de reportes (Excel y auditorÃ­a). ğŸ“¥ğŸ’¾ğŸ“ŠğŸ“‹

* **TransformaciÃ³n y Limpieza (EA2):**  
  SimulaciÃ³n de datos sucios, limpieza, transformaciÃ³n, exportaciÃ³n de los CSV con datos limpios y sucios, generaciÃ³n de un informe de auditorÃ­a detallado y actualizaciÃ³n de la base de datos (nuevas tablas `covid_data_cleaned` y `covid_data_dirty`). ğŸ”„ğŸ§¹ğŸ“ˆğŸ“ğŸ“

* **Enriquecimiento (EA3):**  
  IntegraciÃ³n de datos externos (Delitos InformÃ¡ticos e Inventario Bovinos), combinaciÃ³n con el dataset limpio de COVID-19, generaciÃ³n de un dataset enriquecido y un reporte de auditorÃ­a. ğŸŒğŸ“ˆğŸ“

* **AutomatizaciÃ³n:**  
  IntegraciÃ³n con GitHub Actions ğŸ¤– y ejecuciÃ³n en contenedores Docker ğŸ³.
<br><br>

Â¡Felicidades! Has implementado exitosamente un sistema de ingesta, limpieza y enriquecimiento de datos, simulando una plataforma Big Data en la nube. ğŸ‰

---

## 13. Autores

<div align="center">
  <img src="https://www.iudigital.edu.co/images/11.-IU-DIGITAL.png" alt="IU Digital" width="350">

  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

  <h1>ğŸ“‹ Evidencia de Aprendizaje 3<br>
  <sub>Enriquecimiento de Datos simulando una Plataforma de Big Data en la Nube<sub></h1>
  <h3>Parte 1, 2 y 3 del Proyecto Integrador</h3>

  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

  ### ğŸ‘¥ **Integrantes**
  **Jhon Alexis Machado RodrÃ­guez**  
  **JuliÃ¡n JosÃ© MartÃ­nez Camacho**

  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

  #### ğŸ“ **IngenierÃ­a de Software y Datos**  
  #### ğŸ›ï¸ InstituciÃ³n Universitaria Digital de Antioquia  
  #### ğŸ“… Semestre 9Â°  
  #### ğŸ“¦ Infraestructura y arquitectura para Big Data  
  #### ğŸ”– PREICA2501B010112  
  #### ğŸ‘¨â€ğŸ«ğŸ« andres felipe callejas  

  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

  **ğŸ—“ 26 de Marzo del 2025**  

</div>

---